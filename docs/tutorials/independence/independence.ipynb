{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "# custom seaborn plot options to make the figures pretty\n",
    "sns.set(color_codes=True, style='white', context='talk', font_scale=1)\n",
    "PALETTE = sns.color_palette(\"Set1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independence Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discovering and deciphering relationships in data is often a difficult and opaque process. Identifying causality among variables is particularly important since it allows us to know which relationships to investigate further. This problem, called the independence testing problem, can be defined as follows: Consider random variables $X$ and $Y$ that have joint density $F_{XY} = F_{X|Y} F_Y$. The null hypothesis is thus that $X$ and $Y$ are independently distributed from one another; in other words, $F_{XY} = F_X F_Y$. The alternate hypothesis is that the $X$ and $Y$ are not indepenently distributed from on another. That is,\n",
    "\n",
    "\\begin{align*}\n",
    "H_0: F_{XY} &= F_X F_Y \\\\\n",
    "H_A: F_{XY} &\\neq F_X F_Y\n",
    "\\end{align*}\n",
    "\n",
    "Fortunately, `hyppo` provides an easy-to-use structure to use tests such as these. All tests have a class associated with it and have a `.test` method. The nuances of each test will be examined below; for the sake of this tutorial, the `MGC` class will be used.\n",
    "\n",
    "Importing tests from `hyppo` is similar to importing functions/classes from other packages. `hyppo` also has a `sims` module, which contains several linear and nonlinear dependency structures to test the cases for which each test will perform best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyppo.independence import MGC, Dcorr\n",
    "from hyppo.sims import linear, spiral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate some simulated data. The independence simulations included take on required parameters number of samples and number of dimensions with optional paramaters mentioned in the [reference](https://hyppo.neurodata.io/reference/sims.html#independence-simulations) section of the docs. Looking at some linearly distributed data first, let's look at 100 samples of noisey data and generate a unidimensional `x` and `y` (a 1000 sample of no noise simulated data shows the trend in the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyppo code used to produce the simulation data\n",
    "x, y = linear(100, 1, noise=True)\n",
    "x_no_noise, y_no_noise = linear(1000, 1, noise=False)\n",
    "\n",
    "\n",
    "# stuff to make the plot and make it look nice\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = sns.scatterplot(x=x[:,0], y=y[:,0])\n",
    "ax = sns.scatterplot(x=x_no_noise[:,0], y=y_no_noise[:,0], alpha=0.5)\n",
    "ax.set_xlabel('Simulated X')\n",
    "ax.set_ylabel('Simulated Y') \n",
    "plt.title(\"Linear\")\n",
    "plt.axis('equal')\n",
    "plt.xlim([-1.5, 1.5])\n",
    "plt.ylim([-1.5, 1.5])\n",
    "plt.xticks([-1, 0, 1])\n",
    "plt.yticks([-1, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test statistic the p-value is calculated by running the `.test` method. Some important parameters for the `.test` method:\n",
    "\n",
    "  - `reps`: The number of replications to run when running a permutation test\n",
    "  - `workers`: The number of cores to parallelize over when running a permutation test\n",
    "\n",
    "Note that when using a permutation test, the lowest p-value is the reciprocal of the number of repetitions. Also, since the p-value is calculated with a random permutation test, there will be a slight variance in p-values with subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test statistic and p-values can be calculated for MGC, but a little slow ...\n",
    "stat, pvalue, mgc_dict = MGC().test(x, y)\n",
    "\n",
    "print(\"MGC test statistic:\", stat)\n",
    "print(\"MGC p-value:\", pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so fewer reps can be used, while giving a less confident p-value ...\n",
    "stat, pvalue, mgc_dict = MGC().test(x, y, reps=100)\n",
    "\n",
    "print(\"MGC test statistic:\", stat)\n",
    "print(\"MGC p-value:\", pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or the parallelization can be used (-1 uses all cores)\n",
    "stat, pvalue, mgc_dict = MGC().test(x, y, workers=-1)\n",
    "\n",
    "print(\"MGC test statistic:\", stat)\n",
    "print(\"MGC p-value:\", pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique to MGC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MGC has a few more outputs stored conviently in `mgc_dict`. The contents of this are:\n",
    "\n",
    "  - `mgc_map`: A representation of the latent geometry corresponding to the relationship between the inputs.\n",
    "  - `opt_scale`: The estimated optimal scale as a `(x, y)` pair.\n",
    "  - `null_dist`: The null distribution generated via permutation\n",
    "\n",
    "The `mgc_map` calculates the test statistic over $k$ and $l$ nearest neighbors and the smoothed maximum of those test statistics corresponds to the optimal scale. This is shown in the plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store mgc outputs as variables\n",
    "mgc_map = mgc_dict[\"mgc_map\"]\n",
    "opt_scale = mgc_dict[\"opt_scale\"]\n",
    "\n",
    "print(\"Optimal Scale:\", opt_scale)\n",
    "fig, (ax, cax) = plt.subplots(ncols=2, figsize=(9.45, 7.5),  gridspec_kw={\"width_ratios\":[1, 0.05]})\n",
    "\n",
    "# draw heatmap and colorbar\n",
    "ax = sns.heatmap(mgc_map, cmap=\"YlGnBu\", ax=ax, cbar=False)\n",
    "fig.colorbar(ax.get_children()[0], cax=cax, orientation=\"vertical\")\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# optimal scale\n",
    "ax.scatter(opt_scale[0], opt_scale[1], marker='X', s=200, color='red') \n",
    "\n",
    "# make plots look nice\n",
    "fig.suptitle(\"MGC Map\", fontsize=17)\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "ax.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "ax.yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "ax.set_xlabel('Neighbors for x')\n",
    "ax.set_ylabel('Neighbors for y') \n",
    "ax.set_xticks([0, 50, 100])\n",
    "ax.set_yticks([0, 50, 100])\n",
    "ax.xaxis.set_tick_params()\n",
    "ax.yaxis.set_tick_params()\n",
    "cax.xaxis.set_tick_params()\n",
    "cax.yaxis.set_tick_params()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal scale is shown on the plot as a red \"x\". For a linear relationship like this one, the optimal scale is equivalent to the global scale, which is $(n, n)$. We can use the same thing for nonlinear data as well (spiral in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyppo code used to produce the simulation data\n",
    "x, y = spiral(100, 1, noise=True)\n",
    "x_no_noise, y_no_noise = spiral(1000, 1, noise=False)\n",
    "\n",
    "\n",
    "# stuff to make the plot and make it look nice\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = sns.scatterplot(x=x[:,0], y=y[:,0])\n",
    "ax = sns.scatterplot(x=x_no_noise[:,0], y=y_no_noise[:,0], alpha=0.5)\n",
    "ax.set_xlabel('Simulated X')\n",
    "ax.set_ylabel('Simulated Y') \n",
    "plt.title(\"Spiral\")\n",
    "plt.axis('equal')\n",
    "plt.xticks([-4, 0, 4])\n",
    "plt.yticks([-4, 0, 4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, pvalue, _ = MGC().test(x, y)\n",
    "\n",
    "print(\"MGC test statistic:\", stat)\n",
    "print(\"MGC p-value:\", pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store mgc outputs as variables\n",
    "mgc_map = mgc_dict[\"mgc_map\"]\n",
    "opt_scale = mgc_dict[\"opt_scale\"]\n",
    "\n",
    "print(\"Optimal Scale:\", opt_scale)\n",
    "fig, (ax, cax) = plt.subplots(ncols=2, figsize=(9.45, 7.5),  gridspec_kw={\"width_ratios\":[1, 0.05]})\n",
    "\n",
    "# draw heatmap and colorbar\n",
    "ax = sns.heatmap(mgc_map, cmap=\"YlGnBu\", ax=ax, cbar=False)\n",
    "fig.colorbar(ax.get_children()[0], cax=cax, orientation=\"vertical\")\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# optimal scale\n",
    "ax.scatter(opt_scale[0], opt_scale[1], marker='X', s=200, color='red') \n",
    "\n",
    "# make plots look nice\n",
    "fig.suptitle(\"MGC Map\", fontsize=17)\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "ax.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "ax.yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "ax.set_xlabel('Neighbors for x')\n",
    "ax.set_ylabel('Neighbors for y') \n",
    "ax.set_xticks([0, 50, 100])\n",
    "ax.set_yticks([0, 50, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a unique MGC-map and the optimal scale has shifted! This is because for nonlinear relationships like this one, smaller $k$ and $l$ nearest neighbors more strongly define the relationship. As such, MGC sets the optimal scale to be local rather than global from before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique to Dcorr/Hsic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dcorr and Hsic have a fast chi-square approximation that allows for quick calculation of the p-values without significant loss in statistical power. This can be called using setting `auto = True` when running the `.test` method, which runs the fast chi-square approximation whenever the sample size is larger than 20. Due to the implementation, the values of `reps` and `parameters` mentioned before are irrelevant. By default, the `auto` parameter is set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, pvalue = Dcorr().test(x, y, reps=10000, auto=False)\n",
    "\n",
    "print(\"Dcorr test statistic:\", stat)\n",
    "print(\"Slow Dcorr p-value:\", pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, pvalue = Dcorr().test(x, y, auto=True)\n",
    "\n",
    "print(\"Dcorr test statistic:\", stat)\n",
    "print(\"Fast Dcorr p-value:\", pvalue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
